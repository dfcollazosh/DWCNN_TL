{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "name": "Main_DWCNN_BN_MI_Giga_new_TL2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CI2XchS1aD_o"
      },
      "source": [
        "# Load topographic map data from subjects"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eGQPmXocJCiU"
      },
      "source": [
        "#-------------------------------------------------------------------------------\n",
        "FILEID = \"12Atu_t9xpPChdd1hhKNv2RCiyAVdyUud\"\n",
        "!wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id='$FILEID -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=\"$FILEID -O GIGAnew_data_all_sbjs.zip && rm -rf /tmp/cookies.txt\n",
        "!unzip GIGAnew_data_all_sbjs.zip\n",
        "!dir\n",
        "!pip install mne==0.19\n",
        "#-------------------------------------------------------------------------------"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BnH1qSIcaJP-"
      },
      "source": [
        "# Load pre-trained weights (Source-single)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9I3OAteTJGtt"
      },
      "source": [
        "#-------------------------------------------------------------------------------\n",
        "FILEID = \"1e8NrT4jCSGC_9LWYH0zkfAa1CxkIpD0J\"\n",
        "!wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id='$FILEID -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=\"$FILEID -O GIGAnew_weights_all_sbjs3.zip && rm -rf /tmp/cookies.txt\n",
        "!unzip GIGAnew_weights_all_sbjs3.zip\n",
        "!dir\n",
        "!pip install mne==0.19\n",
        "#-------------------------------------------------------------------------------"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9zwRZSzz268h"
      },
      "source": [
        "# Loading supporting modules"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FC4_NPbOQEy9",
        "trusted": true
      },
      "source": [
        "#-------------------------------------------------------------------------------\n",
        "import numpy as np\n",
        "import scipy.io as sio\n",
        "import pywt\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import math\n",
        "import shutil\n",
        "import pickle\n",
        "import os\n",
        "import random\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import confusion_matrix,cohen_kappa_score,mean_squared_error\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.constraints import max_norm\n",
        "from datetime import date, datetime\n",
        "from scipy.stats import kurtosis, skew\n",
        "from sklearn.model_selection import ShuffleSplit\n",
        "from time import time\n",
        "#-------------------------------------------------------------------------------"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HeXdEwwE3WZh"
      },
      "source": [
        "# Define custom functions to use\n",
        "- We design three custom functions: i) load data acroos time windows, ii) normalization function, and iii) function to build the CNN network."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qOU9xkneQEzC",
        "trusted": true
      },
      "source": [
        "#-------------------------------------------------------------------------------\n",
        "def TW_data(sbj,time_inf,time_sup):\n",
        "    # Load data/images----------------------------------------------------------\n",
        "    path_cwt = 'CWT_CSP_data_mubeta_8_30_Tw_'+str(time_inf)+'s_'+str(time_sup)+'s_subject'+str(sbj)+'_cwt_resized_10.pickle'  \n",
        "    with open(path_cwt, 'rb') as f:\n",
        "         X_train_re_cwt, X_test_re_cwt, y_train, y_test = pickle.load(f)\n",
        "    path_csp = 'CWT_CSP_data_mubeta_8_30_Tw_'+str(time_inf)+'s_'+str(time_sup)+'s_subject'+str(sbj)+'_csp_resized_10.pickle'  \n",
        "    with open(path_csp, 'rb') as f:\n",
        "         X_train_re_csp, X_test_re_csp, y_train, y_test = pickle.load(f)\n",
        "    #---------------------------------------------------------------------------\n",
        "    return X_train_re_cwt, X_train_re_csp, X_test_re_cwt, X_test_re_csp, y_train, y_test\n",
        "#-------------------------------------------------------------------------------\n",
        "def norm_data(XF_train_cwt, XF_train_csp, XF_test_cwt, XF_test_csp, n_fb, Ntw, y_train, y_test):\n",
        "    # orden de las inputs:\n",
        "    # [CWT_fb1_TW1, CWT_fb2_TW1 --- CWT_fb1_TW2, CWT_fb2_TW2 --- CWT_fb1_TWN, CWT_fb2_TWN] ... [CSP]\n",
        "    #---------------------------------------------------------------------------\n",
        "    XT_train_csp = []\n",
        "    XT_valid_csp = []\n",
        "    XT_test_csp  = []\n",
        "    XT_train_cwt = []\n",
        "    XT_valid_cwt = []\n",
        "    XT_test_cwt  = []\n",
        "    for tw in range(Ntw):\n",
        "        for fb in range(n_fb):\n",
        "            X_train_cwt, X_test_cwt = XF_train_cwt[tw][:,fb,:,:].astype(np.uint8), XF_test_cwt[tw][:,fb,:,:].astype(np.uint8)\n",
        "            X_train_csp, X_test_csp = XF_train_csp[tw][:,fb,:,:].astype(np.uint8), XF_test_csp[tw][:,fb,:,:].astype(np.uint8)\n",
        "            #-------------------------------------------------------------------\n",
        "            # train/validation data split\n",
        "            rs = ShuffleSplit(n_splits=1, test_size=.1, random_state=0)\n",
        "            for train_index, valid_index in rs.split(X_train_cwt):\n",
        "              X_train_cwtf = X_train_cwt[train_index,:,:] # cwt\n",
        "              X_valid_cwtf = X_train_cwt[valid_index,:,:]\n",
        "              X_train_cspf = X_train_csp[train_index,:,:] # csp\n",
        "              X_valid_cspf = X_train_csp[valid_index,:,:]\n",
        "            #-------------------------------------------------------------------          \n",
        "            # Normalize data----------------------------------------------------\n",
        "            X_mean_cwt  = X_train_cwtf.mean(axis=0, keepdims=True)\n",
        "            X_std_cwt   = X_train_cwtf.std(axis=0, keepdims=True) + 1e-7\n",
        "            X_train_cwt = (X_train_cwtf - X_mean_cwt) / X_std_cwt\n",
        "            X_valid_cwt = (X_valid_cwtf - X_mean_cwt) / X_std_cwt\n",
        "            X_test_cwt  = (X_test_cwt  - X_mean_cwt) / X_std_cwt\n",
        "                                \n",
        "            X_mean_csp  = X_train_cspf.mean(axis=0, keepdims=True)\n",
        "            X_std_csp   = X_train_cspf.std(axis=0, keepdims=True) + 1e-7\n",
        "            X_train_csp = (X_train_cspf - X_mean_csp) / X_std_csp\n",
        "            X_valid_csp = (X_valid_cspf - X_mean_csp) / X_std_csp\n",
        "            X_test_csp  = (X_test_csp  - X_mean_csp) / X_std_csp\n",
        "            #-------------------------------------------------------------------\n",
        "            # set new axis------------------------------------------------------\n",
        "            X_train_cwt = X_train_cwt[..., np.newaxis]\n",
        "            X_valid_cwt = X_valid_cwt[..., np.newaxis]\n",
        "            X_test_cwt  = X_test_cwt[..., np.newaxis]   \n",
        "            XT_train_cwt.append(X_train_cwt)\n",
        "            XT_valid_cwt.append(X_valid_cwt)\n",
        "            XT_test_cwt.append(X_test_cwt)\n",
        "                                \n",
        "            X_train_csp = X_train_csp[..., np.newaxis]\n",
        "            X_valid_csp = X_valid_csp[..., np.newaxis]\n",
        "            X_test_csp  = X_test_csp[..., np.newaxis]   \n",
        "            XT_train_csp.append(X_train_csp)\n",
        "            XT_valid_csp.append(X_valid_csp)\n",
        "            XT_test_csp.append(X_test_csp)\n",
        "            #-------------------------------------------------------------------\n",
        "    y_trainf = y_train[train_index]\n",
        "    y_validf = y_train[valid_index]\n",
        "    y_trainF, y_validF, y_testF = y_trainf.reshape((-1,))-1, y_validf.reshape((-1,))-1, y_test.reshape((-1,))-1\n",
        "    #---------------------------------------------------------------------------\n",
        "    # Convert class vectors to binary class matrices----------------------------\n",
        "    y_train = keras.utils.to_categorical(y_trainF,num_classes)\n",
        "    y_valid = keras.utils.to_categorical(y_validF,num_classes)\n",
        "    y_test  = keras.utils.to_categorical(y_testF,num_classes)\n",
        "    #---------------------------------------------------------------------------\n",
        "    XT_train = XT_train_cwt + XT_train_csp\n",
        "    XT_valid = XT_valid_cwt + XT_valid_csp\n",
        "    XT_test  = XT_test_cwt  + XT_test_csp\n",
        "    #---------------------------------------------------------------------------\n",
        "    return XT_train, XT_valid, XT_test, y_train, y_valid, y_test \n",
        "#-------------------------------------------------------------------------------\n",
        "def cnn_network(n_fb,Nkfeats,Ntw,shape_,n_filt,units,l1p,l2p,lrate,sbj):\n",
        "    #---------------------------------------------------------------------------\n",
        "    keras.backend.clear_session()\n",
        "    np.random.seed(123)\n",
        "    tf.compat.v1.random.set_random_seed(123)\n",
        "    #---------------------------------------------------------------------------\n",
        "    input_  = [None]*Ntw*n_fb*Nkfeats\n",
        "    conv_   = [None]*Ntw*n_fb*Nkfeats\n",
        "    pool_   = [None]*Ntw*n_fb*Nkfeats\n",
        "    batch0_ = [None]*Ntw*n_fb*Nkfeats\n",
        "    for i in range(Ntw*n_fb*Nkfeats):\n",
        "        input_[i]  = keras.layers.Input(shape=[shape_,shape_,1])\n",
        "        conv_[i]   = keras.layers.Conv2D(filters=n_filt,kernel_size=3,strides=1,activation='relu',padding='SAME',input_shape=[shape_,shape_,1])(input_[i])\n",
        "        #-----------------------------------------------------------------------\n",
        "        batch0_[i] = keras.layers.BatchNormalization()(conv_[i])\n",
        "        #-----------------------------------------------------------------------\n",
        "        pool_[i]   = keras.layers.MaxPooling2D(pool_size=2)(batch0_[i])\n",
        "    concat  = keras.layers.concatenate(pool_)\n",
        "    flat    = keras.layers.Flatten()(concat)\n",
        "    batch1  = keras.layers.BatchNormalization()(flat)\n",
        "    hidden1 = keras.layers.Dense(units=units,activation='relu',kernel_regularizer=keras.regularizers.l1_l2(l1=l1p, l2=l2p), kernel_constraint=max_norm(1.))(batch1)#\n",
        "    batch2  = keras.layers.BatchNormalization()(hidden1)\n",
        "    output  = keras.layers.Dense(units=2, activation='softmax', kernel_constraint=max_norm(1.))(batch2)#\n",
        "    model   = keras.models.Model(inputs=input_, outputs=[output])\n",
        "    \n",
        "    learning_rate_fn = tf.keras.optimizers.schedules.PolynomialDecay(lrate, 100, power=1.0,cycle=False, name=None)\n",
        "    opt     = keras.optimizers.Adam(learning_rate=learning_rate_fn) \n",
        "    model.compile(loss='mean_squared_error', optimizer=opt, metrics=['accuracy'])\n",
        "    return model\n",
        "#-------------------------------------------------------------------------------"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9syBjb784U7J"
      },
      "source": [
        "# Creating results and parameter setting folders\n",
        "- In folder results we store the performance values in format .mat\n",
        "- In folder parameter_setting we store the model' weights in format .hdf5 per fold."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fWIGRR3NQXh1",
        "trusted": true
      },
      "source": [
        "#-------------------------------------------------------------------------------\n",
        "try:\n",
        "  os.mkdir('results')\n",
        "except:\n",
        "  print(\"Exist folder!\")\n",
        "#-------------------------------------------------------------------------------\n",
        "#-------------------------------------------------------------------------------\n",
        "try:\n",
        "  os.mkdir('parameter_setting')\n",
        "except:\n",
        "  print(\"Exist folder!\")\n",
        "#-------------------------------------------------------------------------------"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iut4UbNq5Ms_"
      },
      "source": [
        "# DWCNN training, validation and test stage using pre-trained weights from source subject to initilize the model\n",
        "In this example, we consider one fold training and validation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-HJ9cUuBQEzG",
        "trusted": true
      },
      "source": [
        "# Some initial parameters-------------------------------------------------------\n",
        "sbj           = 40                             # subject id (target)\n",
        "tr_sbj        = 36                             # subject id (source)\n",
        "n_fb          = 2                              # 0 --> 8-12Hz, 1--->12-30Hz\n",
        "num_classes   = 2                              # num of classes\n",
        "n_filt        = 2                              # number of CNN filters\n",
        "learning_rate = [1e-3]                         # learning rate value\n",
        "l1_param      = [0.0005,0.001,0.005]           # l_1 regularization param\n",
        "l2_param      = [0.0005,0.001,0.005]           # L_2 regularization param\n",
        "Ntw           = 5                              # number of time windows\n",
        "acc_test      = []                             # store variable of acc validation\n",
        "Nkfeats       = 2                              # number of feat types\n",
        "#-------------------------------------------------------------------------------\n",
        "# load data train/test trough all tw\n",
        "th_name  = np.array([[-1.5, 0.5],[-0.5, 1.5],[0.5, 2.5],[1.5, 3.5],[2.5, 4.5]]) # considered time-windows\n",
        "XF_train_cwt = []\n",
        "XF_train_csp = []\n",
        "XF_test_cwt  = []\n",
        "XF_test_csp  = []\n",
        "for i in range(th_name.shape[0]):\n",
        "  X_train_re_cwt, X_train_re_csp, X_test_re_cwt, X_test_re_csp, y_trainF, y_testF = TW_data(sbj,th_name[i,0],th_name[i,1])\n",
        "  XF_train_cwt.append(X_train_re_cwt)\n",
        "  XF_train_csp.append(X_train_re_csp)\n",
        "  XF_test_cwt.append(X_test_re_cwt)\n",
        "  XF_test_csp.append(X_test_re_csp)\n",
        "#-------------------------------------------------------------------------------\n",
        "# adjust data\n",
        "XT_train, XT_valid, XT_test, y_train, y_valid, y_test = norm_data(XF_train_cwt, XF_train_csp, XF_test_cwt, XF_test_csp, n_fb, Ntw, y_trainF, y_testF)\n",
        "print('Train', XT_train[0].shape,y_train.shape)\n",
        "print('Valid', XT_valid[0].shape,y_valid.shape)\n",
        "print('Test', XT_test[0].shape,y_test.shape)\n",
        "#-------------------------------------------------------------------------------\n",
        "files_     = os.listdir(\"./\")\n",
        "files_sbj_ = [s for s in files_ if 'sbj_'+str(tr_sbj)+'_' in s]\n",
        "print(files_sbj_)\n",
        "id_sbj  = files_sbj_[0].find('units_')\n",
        "id_l1   = files_sbj_[0].find('_l1_')\n",
        "n_unit  = int(files_sbj_[0][id_sbj+6:id_l1])\n",
        "#-------------------------------------------------------------------------------\n",
        "start = time()\n",
        "for lr_p in range(len(learning_rate)):# loop across lr param\n",
        "  for l1_p in range(len(l1_param)):# loop across l1 param\n",
        "      for l2_p in range(len(l2_param)):# loop across l2 param\n",
        "          #---------------------------------------------------------------------\n",
        "          # define cnn network\n",
        "          model = cnn_network(n_fb,Nkfeats,Ntw,40,n_filt,n_unit,l1_param[l1_p],l2_param[l2_p],learning_rate[lr_p],sbj)\n",
        "          #---------------------------------------------------------------------\n",
        "          # load learned weights\n",
        "          filepath0        = files_sbj_[0]\n",
        "          checkpoint_path0 = filepath0\n",
        "          model.load_weights(checkpoint_path0)\n",
        "          #---------------------------------------------------------------------\n",
        "          # checkpoint\n",
        "          filepath   ='parameter_setting/weights_sbj_'+str(sbj)+'_filters_'+str(n_filt)+'_units_'+str(n_unit)+'_lr_'+str(learning_rate[lr_p])+'_l1_'+str(l1_param[l1_p])+'_l2_'+str(l2_param[l2_p])+'_source_'+str(tr_sbj)+'.hdf5'\n",
        "          checkpoint = keras.callbacks.ModelCheckpoint(filepath, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\n",
        "          #---------------------------------------------------------------------\n",
        "          # Fit the CNN model\n",
        "          history = model.fit(XT_train, y_train, \n",
        "                                epochs=200, \n",
        "                                batch_size=256, \n",
        "                                validation_data=(XT_valid,y_valid),\n",
        "                                callbacks=[checkpoint, keras.callbacks.EarlyStopping(patience=200,restore_best_weights=True)],verbose=2)\n",
        "          #---------------------------------------------------------------------\n",
        "          # Predict model and confusion matrix\n",
        "          model.load_weights(filepath)\n",
        "          y_test_pred = model.predict(XT_test)\n",
        "          y_test_pred = np.argmax(y_test_pred, axis=1)\n",
        "          y_test1     = np.argmax(y_test,axis=1)\n",
        "          AccV        = 1-mean_squared_error(y_test1,y_test_pred)\n",
        "          kappaV      = cohen_kappa_score(y_test1, y_test_pred, labels=None, weights=None)\n",
        "          #---------------------------------------------------------------------      \n",
        "          # plot loss performance    \n",
        "          plt.plot(history.history['loss'])\n",
        "          plt.plot(history.history['val_loss'])\n",
        "          plt.title('model loss')\n",
        "          plt.ylabel('loss')\n",
        "          plt.xlabel('epoch')\n",
        "          plt.legend(['train', 'valid'], loc='upper left')\n",
        "          plt.show()\n",
        "          #---------------------------------------------------------------------\n",
        "          with open('parameter_setting/tr_vld_loss_sbj_'+str(sbj)+'_units_'+str(n_unit)+'_lr_'+str(learning_rate[lr_p])+'_l1_'+str(l1_param[l1_p])+'_l2_'+str(l2_param[l2_p])+'_source_'+str(tr_sbj)+'.pickle', 'wb') as f:\n",
        "            pickle.dump([history.history['loss'], history.history['val_loss']], f)\n",
        "          #---------------------------------------------------------------------\n",
        "          del model\n",
        "          #---------------------------------------------------------------------\n",
        "          acc_test.append([sbj,\n",
        "                          n_filt,\n",
        "                          n_unit,\n",
        "                          learning_rate[lr_p],\n",
        "                          l1_param[l1_p],\n",
        "                          l2_param[l2_p],\n",
        "                          AccV*100, \n",
        "                          kappaV])\n",
        "          sio.savemat('results/Sbj_'+str(sbj)+'.mat',{'Acc':acc_test})\n",
        "          #---------------------------------------------------------------------\n",
        "print('CNN train/validation Done!!!\\n')\n",
        "print('Time for CNN training and validation:', time()-start)\n",
        "#-------------------------------------------------------------------------------"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZWEYAfFVR3kJ"
      },
      "source": [
        "# Auxiliar definition"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SfmFO_qpQEzK",
        "trusted": true
      },
      "source": [
        "# definitions\n",
        "#-------------------------------------------------------------------------------\n",
        "def min_dist(X):\n",
        "    p1       = [100.0,1.0]\n",
        "    distance = []\n",
        "    for i in range(len(X)):\n",
        "        p2 = X[i,:]\n",
        "        distance.append(math.sqrt( ((p1[0]-p2[0])**2)+((p1[1]-p2[1])**2) ))\n",
        "    min_id = np.argmin(distance) \n",
        "    return distance, min_id\n",
        "#-------------------------------------------------------------------------------"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xQ5UavkCIP6g"
      },
      "source": [
        "# Cuantitative results (classification performance on test set)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YoK14VEA9y5A",
        "trusted": true
      },
      "source": [
        "#-------------------------------------------------------------------------------\n",
        "Nsbj        = 40\n",
        "n_fb        = 2\n",
        "num_classes = 2\n",
        "print('Subject '+str(Nsbj))\n",
        "# load rithms data--------------------------------------------------------------\n",
        "data = sio.loadmat('results/Sbj_'+str(Nsbj)+'.mat')\n",
        "data = data['Acc']\n",
        "#print(data)\n",
        "X    = data[:,data.shape[1]-2:data.shape[1]]\n",
        "print(X)\n",
        "#-------------------------------------------------------------------------------    \n",
        "# select best performance-------------------------------------------------------\n",
        "distance, min_id = min_dist(X)\n",
        "max_id           = min_id\n",
        "opt_neurons      = data[max_id,data.shape[1]-6]\n",
        "opt_lr           = data[max_id,data.shape[1]-5]\n",
        "opt_l1           = data[max_id,data.shape[1]-4]\n",
        "opt_l2           = data[max_id,data.shape[1]-3]\n",
        "print('Subject '+str(Nsbj)+ '- acc: ' +str(X[max_id,0])+ '- Kappa value: '+str(X[max_id,1])+\n",
        "      ' neurons '+str(int(opt_neurons))+' lr ' +str(opt_lr)+' l1_val '+str(opt_l1)+ ' l2_val '+str(opt_l2))\n",
        "#-------------------------------------------------------------------------------"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YPhn9OH9IkOH"
      },
      "source": [
        "# Qualitative results (Interpretability of results according relevant brain areas)\n",
        "- Reconstructed spatial representations that highlights the most contributing information in all domains (Type of EEG representation, filter band and time window.)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VNeVV9k9Ay-w"
      },
      "source": [
        "#-------------------------------------------------------------------------------\n",
        "def hidden_layer_results(hidden_layer,shape_im):\n",
        "    weights, biases = hidden_layer.get_weights()    \n",
        "    tmp0            = np.sum(np.abs(weights),axis=1)\n",
        "    n_filters       = np.int(tmp0.shape[0]/(shape_im[0]*shape_im[1]))\n",
        "    im              = [] \n",
        "    for i in range(n_filters):\n",
        "        image = []\n",
        "        j     = 0 + i\n",
        "        while j < tmp0.shape[0]:\n",
        "            image.append(tmp0[j])\n",
        "            j = j+(n_filters)\n",
        "        image = np.array(image)\n",
        "        im.append(image.reshape((shape_im)))\n",
        "    im  = np.array(im)\n",
        "    # average according frequency bands\n",
        "    vec_t = np.arange(0,im.shape[0],2)\n",
        "    imF   = []\n",
        "    for l in range(len(vec_t)):\n",
        "        if l == len(vec_t)-1:\n",
        "            imF.append(np.mean(im[vec_t[l]:im.shape[0],:,:],axis=0))\n",
        "        else:\n",
        "            imF.append(np.mean(im[vec_t[l]:vec_t[l+1],:,:],axis=0))\n",
        "    imF = np.array(imF)\n",
        "    return imF\n",
        "#-------------------------------------------------------------------------------"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nYnNWg0gEibA",
        "trusted": true
      },
      "source": [
        "#-------------------------------------------------------------------------------\n",
        "names_x  = [r'-1.5s-0.5s',r'$-0.5s-1.5s$',r'$0.5s-2.5s$',r'$1.5s-3.5s$',r'$2.5s-4.5s$']\n",
        "Ntw      = 5\n",
        "n_fb     = 2\n",
        "Nkfeats  = 2\n",
        "hist     = []\n",
        "print('Subject '+str(Nsbj))\n",
        "# figure plot setting-----------------------------------------------------------\n",
        "fig, axs = plt.subplots(4,5,figsize=(10,5.3))\n",
        "fig.subplots_adjust(hspace = 0.1, wspace=.0001)\n",
        "#-------------------------------------------------------------------------------\n",
        "# Describe CNN model------------------------------------------------------------\n",
        "model = cnn_network(n_fb,Nkfeats,Ntw,40,n_filt,opt_neurons,opt_l1,opt_l2,learning_rate,Nsbj)\n",
        "#-------------------------------------------------------------------------------\n",
        "# plot topographic maps---------------------------------------------------------\n",
        "topo_avg_mu_cwt = []\n",
        "topo_avg_be_cwt = []\n",
        "topo_avg_mu_csp = []\n",
        "topo_avg_be_csp = []\n",
        "for tw in range(Ntw):\n",
        "  if tw == 0:\n",
        "    ids_tw = [tw, tw+1, tw+10, tw+10+1] \n",
        "  else:\n",
        "    ids_tw = [tw*2, tw*2+1, tw*2+10, tw*2+10+1]\n",
        "  print(ids_tw)\n",
        "  XT_mu_cwt = []\n",
        "  XT_be_cwt = []\n",
        "  XT_mu_csp = []\n",
        "  XT_be_csp = []\n",
        "  #-----------------------------------------------------------------------------\n",
        "  filepath        = 'parameter_setting/weights_sbj_'+str(Nsbj)+'_filters_2_units_'+str(int(opt_neurons))+'_lr_'+str(opt_lr)+'_l1_'+str(opt_l1)+'_l2_'+str(opt_l2)+'_source_'+str(tr_sbj)+'.hdf5'\n",
        "  checkpoint_path = filepath\n",
        "  model.load_weights(checkpoint_path)\n",
        "  #-----------------------------------------------------------------------------  \n",
        "  # Hidden layer results\n",
        "  hidden_layer = model.layers[83]\n",
        "  shape_im     = model.layers[79].output_shape[1:3]\n",
        "  im_mean      = hidden_layer_results(hidden_layer,shape_im)\n",
        "  #-----------------------------------------------------------------------------    \n",
        "  # standarsize and normalize\n",
        "  x_mu_cwt   = np.array(im_mean[ids_tw[0],:,:])\n",
        "  x_beta_cwt = np.array(im_mean[ids_tw[1],:,:])\n",
        "  x_mu_csp   = np.array(im_mean[ids_tw[2],:,:])\n",
        "  x_beta_csp = np.array(im_mean[ids_tw[3],:,:])\n",
        "  #-----------------------------------------------------------------------------\n",
        "  topo_avg_mu_cwt.append(np.array(x_mu_cwt))\n",
        "  topo_avg_be_cwt.append(np.array(x_beta_cwt))\n",
        "  topo_avg_mu_csp.append(np.array(x_mu_csp))\n",
        "  topo_avg_be_csp.append(np.array(x_beta_csp))\n",
        "  #-----------------------------------------------------------------------------\n",
        "topo_avg_muT_cwt = np.array(topo_avg_mu_cwt)\n",
        "topo_avg_beT_cwt = np.array(topo_avg_be_cwt)\n",
        "topo_avg_muT_csp = np.array(topo_avg_mu_csp)\n",
        "topo_avg_beT_csp = np.array(topo_avg_be_csp)\n",
        "#-------------------------------------------------------------------------------\n",
        "max_val = max(topo_avg_muT_cwt.max(),topo_avg_beT_cwt.max(),topo_avg_muT_csp.max(),topo_avg_beT_csp.max())\n",
        "print('max_val',max_val)\n",
        "#-------------------------------------------------------------------------------\n",
        "topo_avg_muT_cwt = topo_avg_muT_cwt / max_val \n",
        "topo_avg_beT_cwt = topo_avg_beT_cwt / max_val \n",
        "topo_avg_muT_csp = topo_avg_muT_csp / max_val \n",
        "topo_avg_beT_csp = topo_avg_beT_csp / max_val\n",
        "#-------------------------------------------------------------------------------\n",
        "max_val = max(topo_avg_muT_cwt.max(),topo_avg_beT_cwt.max(),topo_avg_muT_csp.max(),topo_avg_beT_csp.max())\n",
        "print('new_max_val',max_val)\n",
        "#-------------------------------------------------------------------------------\n",
        "min_val = min(topo_avg_muT_cwt.min(),topo_avg_beT_cwt.min(),topo_avg_muT_csp.min(),topo_avg_beT_csp.min())\n",
        "print('min_val',min_val)\n",
        "#-------------------------------------------------------------------------------\n",
        "for vnt in range(Ntw):\n",
        "    axs[0,vnt].imshow(topo_avg_muT_cwt[vnt,:,:],vmin=min_val, vmax=max_val)\n",
        "    axs[1,vnt].imshow(topo_avg_beT_cwt[vnt,:,:],vmin=min_val, vmax=max_val)\n",
        "    axs[2,vnt].imshow(topo_avg_muT_csp[vnt,:,:],vmin=min_val, vmax=max_val)\n",
        "    axs[3,vnt].imshow(topo_avg_beT_csp[vnt,:,:],vmin=min_val, vmax=max_val)\n",
        "    axs[3,vnt].set(xlabel=names_x[vnt])\n",
        "    axs[3,vnt].xaxis.get_label().set_fontsize(15)\n",
        "    if vnt == 0:\n",
        "       axs[0,vnt].set(ylabel=r'$CWT \\mu$')\n",
        "       axs[0,vnt].yaxis.get_label().set_fontsize(15)\n",
        "       axs[1,vnt].set(ylabel=r'$CWT \\beta$')\n",
        "       axs[1,vnt].yaxis.get_label().set_fontsize(15)\n",
        "       axs[2,vnt].set(ylabel=r'$CSP \\mu$')\n",
        "       axs[2,vnt].yaxis.get_label().set_fontsize(15)\n",
        "       axs[3,vnt].set(ylabel=r'$CSP \\beta$')\n",
        "       axs[3,vnt].yaxis.get_label().set_fontsize(15)\n",
        "    fig.suptitle('Subject '+str(Nsbj), fontsize=16)\n",
        "\n",
        "    # hist\n",
        "    hist.append(topo_avg_muT_cwt[vnt,:,:].flatten())\n",
        "    hist.append(topo_avg_beT_cwt[vnt,:,:].flatten())\n",
        "    hist.append(topo_avg_muT_csp[vnt,:,:].flatten())\n",
        "    hist.append(topo_avg_beT_csp[vnt,:,:].flatten())\n",
        "#-------------------------------------------------------------------------------\n",
        "for ax in axs.flat:\n",
        "    ax.label_outer()\n",
        "for ax in axs.flat:\n",
        "    ax.set_xticks([])\n",
        "    ax.set_yticks([])\n",
        "    plt.savefig('results/Topoplots_Subject_'+str(Nsbj)+'T_source_'+str(tr_sbj)+'.svg', format='svg')\n",
        "#-------------------------------------------------------------------------------\n",
        "#plot histograms\n",
        "hist = np.concatenate(hist)\n",
        "hist = hist.flatten()\n",
        "plt.figure(figsize=(5,5))\n",
        "plt.hist(hist,bins=80)\n",
        "plt.grid()\n",
        "#-------------------------------------------------------------------------------"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ypGHlcWlKwu3"
      },
      "source": [
        "**NOTE:**\n",
        "- The files of optimal model, the performance file, and the figures can be downloaded from current session COLAB."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XwSzCsiUIPPx"
      },
      "source": [
        "# Download data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jb2tsuxONb_y"
      },
      "source": [
        "#from google.colab import files\n",
        "#files.download('/content/parameter_setting/weights_sbj_'+str(sbj)+'_filters_'+str(n_filt)+'_units_'+str(int(opt_neurons))+'_lr_'+str(opt_lr)+'_l1_'+str(opt_l1)+'_l2_'+str(opt_l2)+'_source_'+str(tr_sbj)+'.hdf5') "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "204vMS85k4dx"
      },
      "source": [
        "#from google.colab import files\n",
        "#files.download('/content/parameter_setting/tr_vld_loss_sbj_'+str(sbj)+'_units_'+str(int(opt_neurons))+'_lr_'+str(opt_lr)+'_l1_'+str(opt_l1)+'_l2_'+str(opt_l2)+'_source_'+str(tr_sbj)+'.pickle') "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BSkLdIimtzoC"
      },
      "source": [
        "#from google.colab import files\n",
        "#files.download('/content/results/Topoplots_Subject_'+str(Nsbj)+'T_source_'+str(tr_sbj)+'.svg')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}